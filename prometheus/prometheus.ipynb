{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"226ad19d6a33498db2c20704e20fc3a8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_682095ea913244f595cb6e21f9c8a2a1","IPY_MODEL_93e5eca540284df48009780d47a0b3ca","IPY_MODEL_e8066f22838040b7b6b863c5008f66ff"],"layout":"IPY_MODEL_c5d1b6fdee22412f9dbd073d819ac282"}},"682095ea913244f595cb6e21f9c8a2a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e85de600699147ac93c6b34aa0ab5fc7","placeholder":"​","style":"IPY_MODEL_2730ac0214e6496f91bf78e4ead8b362","value":"Loading checkpoint shards: 100%"}},"93e5eca540284df48009780d47a0b3ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_27e051b8bcfa4f3d98bdd8534bfb8bf4","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c8b7a9488704b8d945b90a1dd433144","value":8}},"e8066f22838040b7b6b863c5008f66ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd7a1f8cf6af43a597d684fd31c1fdb8","placeholder":"​","style":"IPY_MODEL_4e17fec0ae2f478eb253b3fb5e0b1f86","value":" 8/8 [01:02&lt;00:00,  6.82s/it]"}},"c5d1b6fdee22412f9dbd073d819ac282":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e85de600699147ac93c6b34aa0ab5fc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2730ac0214e6496f91bf78e4ead8b362":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27e051b8bcfa4f3d98bdd8534bfb8bf4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c8b7a9488704b8d945b90a1dd433144":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd7a1f8cf6af43a597d684fd31c1fdb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e17fec0ae2f478eb253b3fb5e0b1f86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"rm -rf /kaggle/working/*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone --branch main https://github.com/giankev/Ancient-to-Modern-Italian-Automatic-Translation.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"CUDA (GPU) not available, using CPU.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.cuda.device_count())\nprint(torch.cuda.get_device_name(0)) \nprint(torch.cuda.get_device_name(1)) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ngemma2b_it_cl_translation = pd.read_csv(\"/kaggle/working/Ancient-to-Modern-Italian-Automatic-Translation/Gemma2b-it-translation/CulturalI-hw2_transl-Gemma2b-it_context_learning.csv\")\ndataset_gold = pd.read_csv(\"/kaggle/working/Ancient-to-Modern-Italian-Automatic-Translation/dataset/dataset_gold.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_concat = pd.concat([gemma2b_it_cl_translation, dataset_gold], axis=1)  \ndf_concat.rename(columns={'translation': 'response'}, inplace=True)\ndf_concat.rename(columns={'Translation': 'reference_answer'}, inplace=True)\ndf_concat = df_concat[[\"response\",\"reference_answer\"]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ABS_SYSTEM_PROMPT = \"\"\"You are a fair judge assistant tasked with providing clear,\n                     objective feedback based on specific criteria, ensuring each assessment\n                     reflects the absolute standards set for performance.\"\"\"\n\nABSOLUTE_PROMPT = \"\"\"###Task Description:\nAn instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n4. Please do not generate any other opening, closing, and explanations.\n\n###The instruction to evaluate:\n{instruction}\n\n###Response to evaluate:\n{response}\n\n###Reference Answer:\n{reference_answer}\n\n###Score Rubrics:\n{rubric}\n\n###Feedback: \"\"\"\n\ndef build_prometheus_prompt(response_given: str, reference_answer: str):\n    instruction = \"Evaluate the translation from Old Italian to Modern Italian.\"\n    response = response_given\n    reference = reference_answer\n    rubric = \"\"\"Semantic fidelity and grammatical correctness:\n                Score 1: Completely unacceptable translation: irrelevant, incomprehensible, or nonsensical.\n                Score 2: Severe semantic errors, substantial omissions or additions. Defective syntax.\n                Score 3: Mediocre translation with minor semantic mistakes or typos.\n                Score 4: Good translation, faithful and understandable with slight stylistic imperfections.\n                Score 5: Perfect translation: accurate, fluent, coherent, and semantically correct.\"\"\"\n\n    return ABS_SYSTEM_PROMPT + \"\\n\\n\" + ABSOLUTE_PROMPT.format(\n        instruction=instruction,\n        response=response_given,\n        reference_answer=reference_answer,\n        rubric=rubric\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"prometheus-eval/prometheus-7b-v2.0\", device_map = \"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"prometheus-eval/prometheus-7b-v2.0\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluations(df, output_csv_path):\n    \n    results = []\n    for index, item in tqdm(df.iterrows(), total=df.shape[0], desc=\"Evaluations\", unit=\"phrase\"):\n        response = item[\"response\"]\n        reference_answer = item[\"reference_answer\"]\n        prompt_text = build_prometheus_prompt(response, reference_answer)\n        \n        messages = [\n            {\"role\": \"user\", \"content\": prompt_text},\n        ]\n        \n        encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n        \n        \n        if tokenizer.pad_token_id is None:\n           tokenizer.pad_token_id = tokenizer.eos_token_id\n\n        #encodeds = {k: v.to(model.device) for k, v in encodeds.items()}\n\n        with torch.no_grad():\n            generated_ids = model.generate(\n                            encodeds,\n                            max_new_tokens=1000,\n                            do_sample=False,   \n                            temperature=0.0,    \n                            pad_token_id=tokenizer.pad_token_id, \n                            eos_token_id=tokenizer.eos_token_id     \n                        )\n            \n        decoded = tokenizer.batch_decode(generated_ids)\n    \n        score = \"\"\n        if \"[RESULT]\" in decoded[0]:\n          score = decoded[0].split(\"[RESULT]\")[-1].strip().replace(\"</s>\", \"\").strip() \n    \n        if (index + 1) % 10 == 0:\n                print(f\"\\nProcessed {index + 1} phrase.\")\n    \n        results.append({\n                'response': response,\n                'reference_answer': reference_answer,\n                'score': score\n            })\n        \n    df_output = pd.DataFrame(results)\n    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n    df_output.to_csv(output_csv_path, index=False, encoding='utf-8')\n        \n    print(f\"\\nTranslation complete. Results saved in: {output_csv_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_csv_path = \"/kaggle/working/Gemma2b-it-evaluations_prometheus.csv\"\nevaluations(df_concat, output_csv_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/working/Gemma2b-it-evaluations_prometheus.csv\")\ndf.to_json(\"/kaggle/working/Gemma2b-it-evaluations_prometheus.jsonl\", orient=\"records\", lines=True, force_ascii=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}